[
  {
    "objectID": "bioinformatics.html",
    "href": "bioinformatics.html",
    "title": "Bioinformatics",
    "section": "",
    "text": "Background\nGenomic epidemiology aims to understand the emergence and dissemination of high-risk clones within pathogen populations with the ultimate goal of implementing evidence-based interventions to protect public health. High-risk clones are subpopulations/strains/variants of a pathogen that carry risk elements, such as antibiotic resistance or virulence determinants, and thus pose a potential risk to public health.\nThe increased genetic resolution afforded by genomic data is useful from global to local geographic scales, and has proven to be particularly useful for the investigation of pathogens that exhibit little genetic variation (e.g. Wong et al. 2016), and for outbreak investigations (e.g. Hendriksen et al. 2011, Eppinger et al. 2014). During outbreak investigations (and other genomic epidemiology studies), epidemiological data from patients is collected by healthcare professionals. Antimicrobial susceptibility data, species identification, and any further phenotypic or molecular characterization of the isolates is often generated by the laboratories linked to healthcare facilities and/or by the reference laboratory. Ideally, these different sources of data are stored in a centralised surveillance system and database, such as WHONET. However, these systems rarely incorporate genomic data produced by bioinformaticians. Genomic data may include sequence/assembly quality, genotyping/genoserotyping information, presence/absence of known risk elements, and clustering of the isolates based on genetic similarity –usually in the form of a phylogenetic tree or a minimum spanning-tree.\nThe job of a genomic epidemiologist often starts by combining data from diverse sources that might not be complete or standardised to facilitate the identification of relevant patterns for meaningful interpretation. Once integrated, the epi, lab, and genomic data can be interrogated over the structure of the tree to identify clusters of interest that will place the isolates in or out of the outbreak.\nThe Centre for Genomic Pathogen Surveillance develops free web applications for data collection, integration, visualisation, and analysis of genomic epidemiology data. Epicollect5 is a mobile & web application for free and easy data collection. It provides both the web and mobile applications for the generation of forms (questionnaires) and freely hosted project websites for data collection. Projects are created by using the web application at five.epicollect.net, and then downloaded to the device to perform the data collection. Data are collected in the field using multiple devices and all data can be viewed on a central server (via map, tables, and charts). Data-flo is a system for customised integration and manipulation of diverse data via a simple drag and drop interface. Data-flo provides a visual method to design a reusable pipeline to integrate, clean, and manipulate data in a multitude of ways, eliminating the need for continuous manual intervention (e.g., coding, formatting, spreadsheet formulas, manual copy-pasting). Microreact allows you to upload, visualise and explore any combination of clustering (trees), geographic (map) and temporal (timeline) data. Other metadata variables are displayed in a table. You can specify colours and/or shapes to display on the map, tree and/or timeline. A permanent URL is produced for you to share your Microreact.\n\n\nTargeted competencies and Knowledge, Skills and Attitudes\n\nBioinformatics tools\nFrom fastq to report\nIntegrated skills: Scripting, Visualising, Interpreting\n\n\n\nTraining Methods and Instructional Strategies\n\nDiscussions, hands-on activities\n\n\n\nDuration of training\n\nTwo and half days (17h total)\n\n\n\nResources\nA series of resources will be made available for participants prior, during and after the workshop: tutorials / documentation and printed training materials.\n\n\nRecommended background knowledge\n\nBiological and clinical mechanisms underlying infectious pathogens\nInfectious disease epidemiology and surveillance\nDisease outbreaks\nPathogens of clinical significance\nAntimicrobial resistance\nScripting\nExperience with a Linux environment\nKnowledge on next-generation sequencing"
  },
  {
    "objectID": "module1/module1.html",
    "href": "module1/module1.html",
    "title": "SESSION 1 - Digital Epidemiologist’s Toolbox - theory",
    "section": "",
    "text": "Module Leads: Georgina Lewis-Woodhouse, Emmanuelle Kumaran and Monica Abrudan\nThe aim of this module is to introduce you to the Digital Epidemiologist’s toolbox.\n\n\nThe goals of the session will be:\n\nBe able to identify Stakeholders and, their interactions and engagements \nBe able to describe System Architecture\nBe able to describe Data Architecture\nBe able to understand and perform a Gap analysis\n\nYou can find the material for this session at the following link"
  },
  {
    "objectID": "module1/definitions.html",
    "href": "module1/definitions.html",
    "title": "What is Digital Epidemiology?",
    "section": "",
    "text": "The importance of Digital Epidemiology\n“Digital epidemiology is epidemiology building on digital data and tools” (Salathé M. et al, Life Sci Soc Policy. 2018)\n\nFunded by the NIHR under the Global Health Research Unit programme, our work at the Centre for Genomic Pathogen Surveillance focuses specifically on the applications of digital epidemiology of infectious diseases, including outbreak investigations, pathogen surveillance, understanding of vaccine efficacy and spread of antimicrobial resistance genes and vehicles. \nThe importance of digital epidemiology has been raised to a global level during the pandemic. The value of bringing together genomic with epidemiological and clinical data improved public health decision making, informing experts with regard to virus variants expansions and adding significant contributions to the investigation on the origin of the virus. \nMoreover, the pandemic has taught us that the effective use of digital epidemiology can impact on operational decisions in healthcare settings, impacting on patient wellbeing and outcomes.\nDuring the COVID pandemic there has been a learning curve for many laboratories around the world, who were able to use genomic and digital data to good effect, changing operational approaches and decisions based on results. However, interdisciplinary teams formed of wet-lab scientists, Bioinformaticians, clinicians and non-technical staff (hospital managers, for example) often found barriers in communication of information. The relevance and impact of Digital Epidemiology was often difficult to explain between different professional specialisations. Scientists familiar with Digital Epidemiology often found it challenging to communicate to colleagues what genomic data could and could not reveal. \nThere is a general need to learn how to interpret information related to Digital Epidemiology and to better communicate results to various stakeholders."
  },
  {
    "objectID": "module6/microreact/microreact_doc.html",
    "href": "module6/microreact/microreact_doc.html",
    "title": "Microreact documentation",
    "section": "",
    "text": "https://docs.microreact.org/"
  },
  {
    "objectID": "Syllabus.html",
    "href": "Syllabus.html",
    "title": "Digital Epidemiology",
    "section": "",
    "text": "Topics to be addressed during the first two days.\n\n\n\nTraining Objectives\nDuring the first two days, the objectives are to 1) Refresh relevant domain specific knowledge and knowledge of tools frequently used in Digital Epidemiology, 2) Discuss real-case scenarios where digital epidemiology generated actionable data for public health, 3) Analyse GHRU-Nigeria’s stakeholders map, 4) Identify the systems and data architectures 5) Identify strengths, challenges and priorities in Digital Epidemiology and 6) Start a support network for developing strength in Digital Epidemiology.\nLearning objectives. By the end of the first two days, the participants will be able to 1) identify relevant internal and external stakeholders, important to their work environment 2) identify areas of weakness and strength in local digital epidemiology 3) Explain how digital epidemiology can be applied in their own country-context 4) Discuss other real-world scenarios of applied digital epidemiology.\n\n\nTargeted competencies and Knowledge, Skills and Attitudes\n\nDigital Epidemiology data tools and workflows\nFlow of data from sample to decision-making\nIntegration of epidemiological, laboratory and genomic data\nIntegrated skills: Communication, Networking, Reporting\n\n\n\nTraining Methods and Instructional Strategies\n\nDiscussions, hands-on activities\n\n\n\nDuration of training\n\nTwo days (14h total)\n\n\n\nResources\nA series of resources will be made available for participants prior, during and after the workshop: tutorials / documentation and printed training materials.\n\n\nRecommended background knowledge\n\nBiological and clinical mechanisms underlying infectious pathogens\nInfectious disease epidemiology and surveillance\nDisease outbreaks\nPathogens of clinical significance\nAntimicrobial resistance"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Index",
    "section": "",
    "text": "This page is a repository for training resources designed and developed for the Pathogen Genomics Week, Oxford, March 2024.\nLink to Digital Epidemiology\nLink to Bioinformatics\nLink to Trim, Assembly and QC\nLink to Pathogenwatch\nLink to Data-flo\nLink to Microreact\nLink to Resources\nThe materials provided in this repository are FREE to use. This work is licensed under a Creative Commons Attribution 4.0 International License. Reuse is encouraged with acknowledgement."
  },
  {
    "objectID": "module5/data-flo/data-flo_doc.html",
    "href": "module5/data-flo/data-flo_doc.html",
    "title": "Data-flo documentation",
    "section": "",
    "text": "https://docs.data-flo.io/introduction/readme"
  },
  {
    "objectID": "module2/module2.html",
    "href": "module2/module2.html",
    "title": "SESSION 2 - Introduction to the CGPS tools",
    "section": "",
    "text": "Module Leads: Georgina Lewis-Woodhouse and Emmanuelle Kumaran\nThe aim of this module is to introduce you to the tools developed by CGPS.\nYou can find the material for this session at the following links:\n\nEpicollect\nData-flo\nMicroreact"
  },
  {
    "objectID": "module4/module4.html",
    "href": "module4/module4.html",
    "title": "SESSION 4 - Genome Analysis with Pathogenwatch",
    "section": "",
    "text": "Module Leads: Julio Diaz Caballero, Natacha Couto, Georgina Lewis-Woodhouse, Emmanuelle Kumaran, Nabil Fareed-Alikhan, Sophia David, Monica Abrudan\n\nBased on the exercise developed by Silvia Argimon\n\n\n\nTable of contents\n\nIntroduction\nObjectives\nGenome analysis with Pathogenwatch\n\n\n\nIntroduction\nPathogenwatch provides a platform for analysing and comparing pathogen genome assemblies from around the world, integrating diverse data sets with rich representation. It provides species and taxonomy prediction for over 60,000 variants of bacteria, viruses, and fungi as well as analytical pipelines for a number of pathogens of interest.\n\n\nObjectives\nAt the end of this session the participants will be able to:\n\nUpload assemblies to PW\nLook at QC metrics provided by PW\nCreate a collection with the uploaded assemblies\nCreate a collection with the uploaded assemblies plus contextual genomes (Kp ST258 from Jan 2020 until now)\nExplore the genomes in PW\nDownload the cgSNP tree\nDownload the Kleborate results\nDownload the Metadata\n\n\n\nGenome analysis with Pathogenwatch\nhttps://pathogen.watch\nAfter obtaining the high quality assemblies, you will use https://pathogen.watch to determine the species and sequence type (ST) of these genomes, understand the wider context and download the results of the analytics.\nClick on the upload link at the top right of the home page of Pathogenwatch.\n\n\n\n\n\nClick on Single Genome FASTAs as this is what you have as your output from the assemblies.\n\n\n\n\n\nDrag and drop your FASTA files into the browser.\n\n\n\n\n\nYou will see the analysis running and the genomes will be speciated and species specific analytics run.\n\n\n\n\n\nClick on VIEW GENOMES.\n\n\n\n\n\n\nSelect one of the analysed genomes to look at the single genome report. This is where you can look at results of the analysis, any associated metadata and key stats.\n\n\n\n\n\nSelect Klebsiella pneumoniae genomes and click on Selected Genomes to create a new collection.\n\n\n\n\n\nClick on Create Collection and add a title and description for your initial collection of genomes.\n\n\n\n\n\nClick CREATE NOW and you can see your initial genomes in the Collection View.\n\n\n\n\n\nGo back to the Genome View and now we want to investigate Kp ST258 and the wider context from 2020 to present day.\nYou can filter all genomes within Pathogenwatch from the left sidebar.\nFilter the genomes by:\n\nGenus - Klebsiella\nSpecies - Klebsiella pneumoniae\nMLST - ST 258\nDate - 2020 to 2023\n\n\n\n\n\n\nYou will see all genomes that are publicly available for those filters that you have applied.\nCreate a collection with all the genomes that are listed.\n\n\n\n\n\nExplore this wider collection in the Collection View.\n\n\n\n\n\nYou can now download the results as files. Download the Metadata table, Kleborate results and the Tree as a .nwk file.\nYou should now be able to find the following files in your Downloads folder:\nmetadata.csv\nkleborate.csv\ntree.nwk\nThe materials provided in this repository are FREE to use. This work is licensed under a Creative Commons Attribution 4.0 International License. Reuse is encouraged with acknowledgement."
  },
  {
    "objectID": "module3/module3.html#hardware",
    "href": "module3/module3.html#hardware",
    "title": "SESSION 3 - Introduction to Trimming, Assembly and QC",
    "section": "Hardware",
    "text": "Hardware\nFor this exercise you will use our pre configured bioinformatics servers. Connecting to our bioinformatics servers ensure the computational resources required for the assembly process are available. If you wish to run through this module on another platform, you will need to install the required software and download the sample data yourself. The pipeline we will be using is available here https://gitlab.com/cgps/ghru/pipelines/dsl2/pipelines/assembly.\nTo connect to our bioinformatics servers, follow the instructions below. Replace user with your username with the name of the server you want to connect to. You will need to enter your password when prompted. Your password will be provided to you by one of the instructors:\n# Connect to the server\nssh user@159.65.95.44"
  },
  {
    "objectID": "module3/module3.html#data",
    "href": "module3/module3.html#data",
    "title": "SESSION 3 - Introduction to Trimming, Assembly and QC",
    "section": "Data",
    "text": "Data\nYou will find 11 sets of paired-end Illumina reads in the /data/. Thus, this directory will contain 22 fastq files (two per genome). You can check the data is actually there:\n# Check the contents of the /data/ directory\nls /data/\nThe /data directory will serve as the input directory for the GHRU assembly pipeline. You can practice fetching the data from a remote location such as Figshare, ENA or SRA following the instructions in the fetching data bonus section."
  },
  {
    "objectID": "module3/module3.html#software",
    "href": "module3/module3.html#software",
    "title": "SESSION 3 - Introduction to Trimming, Assembly and QC",
    "section": "Software",
    "text": "Software\nThe assembly process pipeline will process each pair of Illumina short-read fastq files and assemble them using the SPAdes assembler. Along the assembly process, the workflow will perform quality control checks and filters. The workflow consists of the following steps:\n\nQC reads using FastQC before trimming\nTrim reads using Trimmomatic and Cutadapt (if --cutadapt specified)\nQC reads using FastQC after trimming\nCorrect reads using lighter\nCheck for contamination using confindr\nCount number of reads and estimate genome size using Mash\nDownsample reads if the --depth_cutoff argument was specified\nMerge reads using Flash where the insert size is small\nAssemble reads using SPAdes (by default the --careful option is turned off)\nAssess species identification using bactinspector\nAssess assembly quality using QUAST\nSummarise all assembly QCs using QUAST\n(Optional if QuailFyr qc conditions YAML file is supplied). Filter assemblies into three directories: pass, warning and failure based on QC metrics\n\n A sumamry of this process is shown above \nNextflow is already installed in our bioinformatics servers, and the assembly pipeline can be found in /opt/ghru-assembly. Let’s check that nextflow is installed and that we can find the assembly pipeline:\n# Check that nextflow is installed\nnextflow -v\n# Check the assembly pipeline can be found at /opt/ghru-assembly directory\nls /opt/ghru-assembly\nNextflow should be preconfigured for you, but if you want to try this on another platform you will need too install nextflow and assembly pipeline. You can install them by following the instructions in the install nextflow or install pipeline bonus sections."
  },
  {
    "objectID": "module3/module3.html#implementation",
    "href": "module3/module3.html#implementation",
    "title": "SESSION 3 - Introduction to Trimming, Assembly and QC",
    "section": "Implementation",
    "text": "Implementation\nFirst we need to create a directory to store the assemblies. We will call this directory results. You can create this directory with the following command, replace user with your username:\n# Create the results directory\nmkdir /workshop/user/results\nNext, let’s create a directory to store the implementation scripts and the working directory. We will call this directory scripts. You can create this directory with the following command, replace user with your username:\n# Create the scripts directory\nmkdir /workshop/user/scripts\nNow we create a script to run the assembly pipeline. You can create a file called assembly.sh in the scripts directory using a text editor such as nano or vim. You can copy and paste the following code into the file and save it:\nPlease change the output directory in line with your username.\n#!/bin/bash\nNEXTFLOW_WORKFLOWS_DIR='/opt/ghru-assembly'\nINPUT_DIR='/data'\nOUTPUT_DIR='/workshop/user/results'\n\nnextflow run ${NEXTFLOW_WORKFLOWS_DIR}/main.nf \\\n    --adapter_file adapters.fas \\\n    --qc_conditions qc_conditions_nextera_relaxed.yml \\\n    --input_dir ${INPUT_DIR} \\\n    --fastq_pattern '*{R,_}{1,2}.f*q.gz' \\\n    --output_dir ${OUTPUT_DIR} \\\n    --depth_cutoff 100 \\\n    --prescreen_file_size_check 12 \\\n    --careful \\\n    --kmer_min_copy 3 \\\n    -w ${OUTPUT_DIR}/work \\\n    -resume\nFinally, you can run the assembly pipeline by executing the script, replace user with your username:\n# Run the assembly pipeline\nbash /workshop/user/scripts/assembly.sh"
  },
  {
    "objectID": "module3/module3.html#results",
    "href": "module3/module3.html#results",
    "title": "SESSION 3 - Introduction to Trimming, Assembly and QC",
    "section": "Results",
    "text": "Results\nThe assembly pipeline will create six directories in the assemblies directory, replace user with your username.\n# Check the contents of the /results directory\nls /workshop/user/results\nYou will find the fasta-formatted assemblies in the assemblies directory. There, you will find pass, warning, and failure directories containing the assemblies according to their quality. You can check the contents of the assemblies directory, replace user with your username:\n# Check the contents of the /results/assemblies directory \nls /workshop/user/results\nAdditonally you will find the quality_reports directory containing the quality control reports for the assemblies. You can check the contents of the quality_reports directory, replace user with your username:\n# Check the contents of the /results/quality_reports directory\nls /workshop/user/results/quality_reports\nYou may want to import the quality report (results/quality_reports/qualifyr_report.tsv) to your local machine so as to explore whether the assemblies are of high quality or not. You can use the scp command to do this, replace user with your username:\n# Copy the quality report to your local machine\nscp user@159.65.95.44:/workshop/user/results/quality_reports/qualifyr_report.tsv /path/to/local/machine\nYou will be prompted for your password. You can use the same command to copy the assemblies to your local machine (hint: scp -r helps you move an entire directory)."
  },
  {
    "objectID": "module3/module3.html#other-workflow-outputs",
    "href": "module3/module3.html#other-workflow-outputs",
    "title": "SESSION 3 - Introduction to Trimming, Assembly and QC",
    "section": "Other workflow outputs",
    "text": "Other workflow outputs\nThere is a great deal of other outputs produced by the pipeline. These will be found in the directory specified by the --output_dir argument. In this case, /workshop/user/results/\n\nA directory called fastqc/post_trimming that contans the Fastqc reports for each fastq in html format after trimming\nA directory called assemblies containing the final assembled scaffold files named as _scaffolds.fasta. If the qc_conditions argument was given there will be subdirectories named pass, warning and failure where the appropiately QCed scaffolds and failure reasons will be stored.\nA directory called quast containing:\n\nA summary quast report named combined_quast_report.tsv\nA transposed summary quast report with the samples as rows so that they can be sorted based on the quast metric in columns named combined_quast_report.tsv\n\nA directory called quality_reports containing html reports\n\nMultiQC summary reports combining QC results for all samples from\n\nFastQC: fastqc_multiqc_report.html\nQuast: quast_multiqc_report.html\n\nQualiFyr reports. If a qc_conditions.yml file was supplied reports will be generated that contain a summary of the overall pass/fail status of each sample.\n\nqualifyr_report.html : e.g QualiFyr Report\nqualifyr_report.tsv\n\n\nif the –full_output parameter is given then the following will also be available in the output directory\n\nA directory called fastqc/pre_trimming that contans the Fastqc reports for each fastq in html format prior to trimming\nA directory called corrected_fastqs that contains the fastq files that have been trimmed with Trimmomatic and corrected using Lighter\nIf using paired end reads, a directory called merged_fastqs that contains the fastq files that have been merged using Flash. There will be a files called\n\n.extendedFrags.fastq.gz merged reads\n.notCombined_1.fastq.gz unmerged read 1 reads\n.notCombined_2.fastq.gz unmerged read 2 reads"
  },
  {
    "objectID": "module3/module3.html#fetching-data",
    "href": "module3/module3.html#fetching-data",
    "title": "SESSION 3 - Introduction to Trimming, Assembly and QC",
    "section": "Fetching data",
    "text": "Fetching data\nYou can use tools such as wget or curl to fetch data from a remote location such as Figshare, ENA or SRA. For example, you can fetch the data from the following Figshare link:\n# Fetch the data from Figshare\nwget https://figshare.com/ndownloader/articles/25266367/versions/1 -O /data/fastqs.zip\n# Unzip the data\nunzip /data/fastqs.zip -d /data"
  },
  {
    "objectID": "module3/module3.html#install-nextflow",
    "href": "module3/module3.html#install-nextflow",
    "title": "SESSION 3 - Introduction to Trimming, Assembly and QC",
    "section": "Install nextflow",
    "text": "Install nextflow\nTo install Nextflow, a tool for orchestrating scientific workflows, follow these steps:\nNextflow requires Java to be installed on your system. Make sure you have Java Development Kit (JDK) version 8 or later installed. You can check if Java is installed by running java -version in your terminal or command prompt.\nThen enter this command in your terminal:\ncurl -s https://get.nextflow.io | bash\nIt creates a file nextflow in the current dir. Visit the nextflow website for more details."
  },
  {
    "objectID": "module3/module3.html#install-pipeline",
    "href": "module3/module3.html#install-pipeline",
    "title": "SESSION 3 - Introduction to Trimming, Assembly and QC",
    "section": "Install assembly pipeline",
    "text": "Install assembly pipeline\nTo install the assembly pipeline, you can clone the repository from the following link:\n# Clone the assembly pipeline\ngit clone https://gitlab.com/cgps/ghru/pipelines/assembly.git\nYou will also need docker installed."
  },
  {
    "objectID": "module3/module3.html#full-list-of-software-used",
    "href": "module3/module3.html#full-list-of-software-used",
    "title": "SESSION 3 - Introduction to Trimming, Assembly and QC",
    "section": "Full list of software used",
    "text": "Full list of software used\nSoftware used within the workflow:\n\nFastQC A quality control tool for high throughput sequence data.\nTrimmomatic A flexible read trimming tool for Illumina NGS data.\nCutadapt Finds and removes adapter sequences, primers, poly-A tails and other types of unwanted sequence from high-throughput sequencing reads\nmash Fast genome and metagenome distance estimation using MinHash.\nlighter Fast and memory-efficient sequencing error corrector.\nseqtk A fast and lightweight tool for processing sequences in the FASTA or FASTQ format.\nFLASH (Fast Length Adjustment of SHort reads) A very fast and accurate software tool to merge paired-end reads from next-generation sequencing experiments.\nSPAdes A genome assembly algorithm designed for single cell and multi-cells bacterial data sets.\ncontig-tools A utility Python package to parse multi fasta files resulting from de novo assembly.\nQuast A tool to evaluate the aulaity of genome assemblies.\nConFindr Software that can detect contamination in bacterial NGS data, both between and within species.\nQualiFyr Software to give an overall QC status for a sample based on multiple QC metric files\nMultiQC Aggregate results from bioinformatics analyses across many samples into a single report\nKAT The K-mer Analysis Toolkit (KAT) contains a number of tools that analyse and compare K-mer spectra\nBactInspector Software using an updated refseq mash database to predict species\n\nThe materials provided in this repository are FREE to use. This work is licensed under a Creative Commons Attribution 4.0 International License. Reuse is encouraged with acknowledgement."
  },
  {
    "objectID": "module5/data-flo/data-flo.html",
    "href": "module5/data-flo/data-flo.html",
    "title": "Data-flo tutorial",
    "section": "",
    "text": "Data-flo (https://data-flo.io/) is a system for customised integration and manipulation of diverse data via a simple drag and drop interface.\nFor Data-flo documentation, go to https://docs.data-flo.io/introduction/readme\nData-flo can easily combine epidemiological data, genomic data, laboratory data, and various metadata from disparate sources (i.e., different data systems) and formats.\nData-flo provides a visual method to design a reusable pipeline to integrate, clean, and manipulate data in a multitude of ways, eliminating the need for continuous manual intervention (e.g., coding, formatting, spreadsheet formulas, manual copy-pasting).\nData-flo pipelines are combinations of ready-to-use data adaptors that can be tailored, modularised and shared for reuse and reproducibility. Once a Data-flo pipeline has been created, it can be run anytime, by anyone with access, to enable push-button data extraction and transformation. This saves significant time by removing the bulk of the manual repetitive workflows that require multiple sequential or tedious steps, enabling practitioners to focus on analysis and interpretation.\n\n\n“Steps” in Data-flo are called ADAPTORS. There are three main types of adaptors, which serve different functions.\n\nImporting data\nManipulating & transforming data\nExporting data\n\n\n\n\nData-flo editing view. Examples of Data-flo adaptors.\n\n\n\n\n\nData-flo editing view. This is an example of a Data-flo that imports data from Google spreadsheets, through “Google spreadsheet” adaptors.\n\n\n\n\n\nData-flo editing view. This is an example of a Data-flo where two datatables are merged using a “Join datatables” adaptor.\n\n\n\n\n\nData-flo editing view. Using various adaptors, columns in a datatable can be removed or renamed, specific strings or blank values can be replaced, dates can be reformatted, distinct lists can be generated.\n\n\n\n\n\nData-flo editing view. In this example, a Data-flo pushes updated data to a Microreact project and supplies the URL for that project using the “Update Microreact project” adaptor.\n\n\n\n\n\n\n\n\nHow to run a Data-flo, Step 1: Go to Data-flo transformations: data-flo.io/transformations/dataflows\n\n\n\n\n\nHow to run a Data-flo, Step 2: Select your favourite Data-flo. The Data-flo shown in this example is “Lab to bioinformatics”\n\n\n\n\n\nHow to run a Data-flo, Step 3: Click on the RUN tab to get to the RUN page.\n\n\n\n\n\nHow to run a Data-flo, Step 4: Hit the RUN button.\n\n\n\n\n\nHow to run a Data-flo, Step 5: Check out the output. In this example, the output is a Microreact link. You can choose to RUN AGAIN the Data-flo, and this will update your results (in case the input had been changed, of course).\n\n\n\n\n\nCreate a Data-flo from scratch, Step 1: Go to the + sign on the bottom right of your screen.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 2: Select “NEW DATAFLOW”.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 3: Select your preferred ACCESS CONTROL.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 4: Add adaptors to your Data-flo. On the left-hand side of Data-flo, you can find the list of available adaptors.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 5: Try to retrieve data from a spreadsheet. Click on “Spreadsheet file” on the list of adaptors. Hover over the “i” button next to “Spreadsheet file” to see basic information about this adaptor.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 6: Clicking on “Spreadsheet file” on the list of adaptors, will add an adaptor (which looks like a box), with the title “Spreadsheet file” on your canvas. When you click on *file on the left side of the adaptor, the view on the right hand side of your screen will change, and you will be able to see a list of options under “BINDING TYPE”, such as “Bind to a Dataflow input” and “Bind to another transformation”.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 7: From the list of “BINDING TYPEs”, Select “Bind to a Dataflow input” and press the “+” next to INPUT ARGUMENT.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 8: A box with “file” will appear on the canvas, linked to the “Spreadsheet file” adaptor.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 9: To run a Data-flo, press the little “bug icon” on top, on the right hand side of the Save button.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 10: Pressing the “bug icon” will trigger the Dataflow Debugger and you will be shown a “Choose file” button.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 11: Press the “Choose file” button and select a spreadsheet from your local computer. Click the “data” on the right hand side of the adaptor.\n\n\n\n\n\nspreadsheet data\n\n\n\n\n\n\n\n\nHow to share your Data-flo, Step 1: Press the downward facing arrow shown here on the top right corner. This will trigger the export of a .json file, which can be sent and shared.\n\n\n\n\n\nHow to share your Data-flo, Step 2: If you have received a .json Data-flo file, press the + sign at the bottom right of your screen\n\n\n\n\n\nHow to share your Data-flo, Step 3: Press the IMPORT button and then select your .json file containing the Data-flo."
  },
  {
    "objectID": "module5/data-flo/data-flo.html#a-short-introduction",
    "href": "module5/data-flo/data-flo.html#a-short-introduction",
    "title": "Data-flo tutorial",
    "section": "",
    "text": "Data-flo (https://data-flo.io/) is a system for customised integration and manipulation of diverse data via a simple drag and drop interface.\nFor Data-flo documentation, go to https://docs.data-flo.io/introduction/readme\nData-flo can easily combine epidemiological data, genomic data, laboratory data, and various metadata from disparate sources (i.e., different data systems) and formats.\nData-flo provides a visual method to design a reusable pipeline to integrate, clean, and manipulate data in a multitude of ways, eliminating the need for continuous manual intervention (e.g., coding, formatting, spreadsheet formulas, manual copy-pasting).\nData-flo pipelines are combinations of ready-to-use data adaptors that can be tailored, modularised and shared for reuse and reproducibility. Once a Data-flo pipeline has been created, it can be run anytime, by anyone with access, to enable push-button data extraction and transformation. This saves significant time by removing the bulk of the manual repetitive workflows that require multiple sequential or tedious steps, enabling practitioners to focus on analysis and interpretation.\n\n\n“Steps” in Data-flo are called ADAPTORS. There are three main types of adaptors, which serve different functions.\n\nImporting data\nManipulating & transforming data\nExporting data\n\n\n\n\nData-flo editing view. Examples of Data-flo adaptors.\n\n\n\n\n\nData-flo editing view. This is an example of a Data-flo that imports data from Google spreadsheets, through “Google spreadsheet” adaptors.\n\n\n\n\n\nData-flo editing view. This is an example of a Data-flo where two datatables are merged using a “Join datatables” adaptor.\n\n\n\n\n\nData-flo editing view. Using various adaptors, columns in a datatable can be removed or renamed, specific strings or blank values can be replaced, dates can be reformatted, distinct lists can be generated.\n\n\n\n\n\nData-flo editing view. In this example, a Data-flo pushes updated data to a Microreact project and supplies the URL for that project using the “Update Microreact project” adaptor.\n\n\n\n\n\n\n\n\nHow to run a Data-flo, Step 1: Go to Data-flo transformations: data-flo.io/transformations/dataflows\n\n\n\n\n\nHow to run a Data-flo, Step 2: Select your favourite Data-flo. The Data-flo shown in this example is “Lab to bioinformatics”\n\n\n\n\n\nHow to run a Data-flo, Step 3: Click on the RUN tab to get to the RUN page.\n\n\n\n\n\nHow to run a Data-flo, Step 4: Hit the RUN button.\n\n\n\n\n\nHow to run a Data-flo, Step 5: Check out the output. In this example, the output is a Microreact link. You can choose to RUN AGAIN the Data-flo, and this will update your results (in case the input had been changed, of course).\n\n\n\n\n\nCreate a Data-flo from scratch, Step 1: Go to the + sign on the bottom right of your screen.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 2: Select “NEW DATAFLOW”.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 3: Select your preferred ACCESS CONTROL.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 4: Add adaptors to your Data-flo. On the left-hand side of Data-flo, you can find the list of available adaptors.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 5: Try to retrieve data from a spreadsheet. Click on “Spreadsheet file” on the list of adaptors. Hover over the “i” button next to “Spreadsheet file” to see basic information about this adaptor.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 6: Clicking on “Spreadsheet file” on the list of adaptors, will add an adaptor (which looks like a box), with the title “Spreadsheet file” on your canvas. When you click on *file on the left side of the adaptor, the view on the right hand side of your screen will change, and you will be able to see a list of options under “BINDING TYPE”, such as “Bind to a Dataflow input” and “Bind to another transformation”.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 7: From the list of “BINDING TYPEs”, Select “Bind to a Dataflow input” and press the “+” next to INPUT ARGUMENT.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 8: A box with “file” will appear on the canvas, linked to the “Spreadsheet file” adaptor.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 9: To run a Data-flo, press the little “bug icon” on top, on the right hand side of the Save button.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 10: Pressing the “bug icon” will trigger the Dataflow Debugger and you will be shown a “Choose file” button.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 11: Press the “Choose file” button and select a spreadsheet from your local computer. Click the “data” on the right hand side of the adaptor.\n\n\n\n\n\nspreadsheet data\n\n\n\n\n\n\n\n\nHow to share your Data-flo, Step 1: Press the downward facing arrow shown here on the top right corner. This will trigger the export of a .json file, which can be sent and shared.\n\n\n\n\n\nHow to share your Data-flo, Step 2: If you have received a .json Data-flo file, press the + sign at the bottom right of your screen\n\n\n\n\n\nHow to share your Data-flo, Step 3: Press the IMPORT button and then select your .json file containing the Data-flo."
  },
  {
    "objectID": "module5/module5.html",
    "href": "module5/module5.html",
    "title": "SESSION 5 - Data integration with Data-flo",
    "section": "",
    "text": "Module Leads: Julio Diaz Caballero, Natacha Couto, Georgina Lewis-Woodhouse, Emmanuelle Kumaran, Nabil Fareed-Alikhan, Sophia David, Monica Abrudan\n\nbased on the exercise developed by Silvia Argimon\n\n\n\nTable of contents\n\nIntroduction\nObjectives\nData integration with Data-flo\nSign-in to data-flo\n\n\n\nIntroduction\nData-flo (https://next.data-flo.io/) is a system for customised integration and manipulation of diverse data via a simple drag and drop interface.**\nFor Data-flo documentation, go to https://docs.data-flo.io/introduction/readme.\nData-flo can easily combine epidemiological data, genomic data, laboratory data, and various metadata from disparate sources (i.e., different data systems) and formats.\nData-flo provides a visual method to design a reusable workflows to integrate, clean, and manipulate data in a multitude of ways, eliminating the need for continuous manual intervention (e.g., coding, formatting, spreadsheet formulas, manual copy-pasting).\nData-flo workflows are combinations of ready-to-use data adaptors that can be tailored, modularised and shared for reuse and reproducibility. Once a Data-flo workflow has been created, it can be run anytime, by anyone with access, to enable push-button data extraction and transformation. This saves significant time by removing the bulk of the manual repetitive workflows that require multiple sequential or tedious steps, enabling practitioners to focus on analysis and interpretation.\n\n\nObjectives\nAt the end of this session the participants will be able to:\n\nBuild a data transformation workflow by combining ready-to-use data adaptors suited to the exercise\nAcquire a Microreact link that combines all the data files from the exercise using Data-flo\n\n\n\nData integration with Data-flo\nhttps://data-flo.io/\nNote: you need to sign-up for Data-flo and Microreact. See instructions in the Resources section. Creating your own account will allow you to manage and edit your projects.\nA cgSNP tree was generated through Pathogenwatch (tree.nwk). Contextual genomes present in Pathogenwatch (Jan 2020-now) were also included in the tree inference and their associated data added to the epi_data.csv file.\nThe user now has the information needed for the investigation in the following formats:\n\nepi_data.csv Epi data from 3 clinical cases, 6 carriers and 11 environmental samples\nlab_results.xlsx Culture and AST results\nPW_metadata.csv metadata results from Pathogenwatch\nPW_Kleborate.csv Kleborate file from Pathogenwatch\ntree.nwk cgSNP tree of 7 Kpn genomes\n\nThe files are located in this link.\nWe will create a Data-flo workflow to consolidate the files and send them to Microreact for visualisation. Follow the steps below:\n1) Start a new workflow: on the Workflows page click the purple on the button right of the screen and select “Create new Workflow”\n\n\n\n\n\n2) This is your canvas. On the left you have the canvas on which you will build your workflow. On the right panel you have the list of all the adaptors you can use to build your Workflow. \n\n\n\n\n\nIt is good practice to start by naming your workflow. To do so click on the “Untitled Workflow” text on the top left.\n3) To import the Epidemiology data, which is in a CSV file, you can use the import from csv file adaptor. You can find it on the left panel by going through the list of adaptors and selecting the appropriate option. Alternatively, you can use the search feature on the top of the adaptor panel to quickly find the adaptor you need. \nOnce you have found the adaptor, click on the file input and select “Data-flo input” on the right panel. This option allows the user to add the CSV file when they run the workflow on the Run page.\nA drop down menu will appear, select “create new input” \n\n\n\n\n\n4) Click on the input data diamond. You can rename the input data name and add a short description to give the user more context as to which file to add. You can also add the CSV to the test run value, this will allow you to test your workflow with a data file as you build it. You can then click the “play” button on the top left of the screen to test run your workflow. It is good practice to do this, each time you add a new adaptor. If the adapter turns red, something has gone wrong with the adaptor setup.\n\n\n\n\n\nAdd latitude and longitudes to the Epidemiological data: \n5) Because the hospital name is not specific, we can use the concatenate column, to merge the City and Hospital columns. Firstly, select concatenate column from the list of adaptors. Then, connect the data output from the “import from csv file” to the data input in the “concatenate column” adaptor. Click on the “columns” option and select “Define value”. Add the names of the two columns you want to merge. In the separator input option on the adaptor, you can also add a separator (for example, a comma) to separate the values from the two concatenated columns. Finally, in the concatenated column input option, add the name of the new column which will contain the concatenated values.\n\n\n\n\n\n6) Add the geocoding adaptor to your workflow. Under the location column input option, enter your concatenated column. This adaptor will create columns named latitude and longitude by default. However, you can choose to name these columns differently by using the latitude column and longitude column input options.\n\n\n\n\n\n7) Import the lab data into your Workflow. You can do this using the import from excel file adaptor. Follow the same instruction for setting up this adaptor as 3) and 4)\n\n\n\n\n\n8) The laboratory data includes samples that are both culture positive and negative. However, for our current workflow, we are only interested in the culture positive samples. To filter out the culture negative samples, we can use the filter rows adaptor. First, we need to specify the name of the column we want to search, which in this case is the culture column. Next, we need to specify the value to be filtered on, for example, Klebsiella pneumoniae. After filtering, we will get two output datatables. One will contain all the rows that include Klebsiella pneumoniae, while the other will contain all the remaining rows.\n\n\n\n\n\n9)To combine the lab data and epidemiology data tables, you can use the join datatables adaptor. First, add the two datatables to the main and other data inputs. Then, specify which columns from each datatable should be used to perform the join. You can choose the type of join you want to perform - in this case, we want an inner join which will only keep the rows that have matching values in both tables. For more information on different types of joins, you can visit the following URL: https://www.datasciencemadesimple.com/join-in-r-merge-in-r/.\n\n\n\n\n\n10) Import the Pathogenwatch metadata using steps 3) and 4)\n11) The metadata file contains several columns that are either blank or not useful. To select only the necessary columns, you can use the select columns adaptor. In the “column names” input, you need to define a list of columns that you want to keep in the file. In this case, you will want to keep the following columns: Name, Country, Collection date, Collected by, Isolation source, Host, Latitude, and Longitude. Note: you can also use the remove columns adaptor, in which case you would need to define the columns you wish to remove. \n\n\n\n\n\n12) Rename the metadata and joined lab and epi datatables in order to make them match. To do so, you can use the rename columns adaptor.\n\n\n\n\n\nRenaming the Metadata datatable: under the column names input, rename all the columns according to the table below. \n\n\n\n\n\nRenaming the joined lab/epi datatable: rename the columns according to the table below. In addition, under the discard unmapped, click the define value and select TRUE, this will discard any columns that were not renamed. \n\n\n\n\n\n13) Back to the PW metadata, you may have observed that the rows associated with the samples from University College Hospital are empty. These rows can be removed as the information for them is present in the joined lab/epi datatable. To do this, you can use the filter rows adaptor. Select the “collection_date” under the column name input and click on “define” under the filter type input. Then choose the “is-blank” option. Note: the data you want will be in the complementary output and not the data output\n\n\n\n\n\n14) Add a country column to the joined lab/epi datatable using the add column adaptor. Under the value input add Nigeria.\n\n\n\n\n\n15) Add a “host” column to the joined lab/epi datatable using the map column values adaptor. Map the values in the “source” column to either “Human” or “Environmental”.\n\n\n\n\n\n16) Append both datatables together using the “append datatables” adaptor\n\n\n\n\n\n17) Import the Kleborate CSV using steps 3) and 4)\n18) Join the Kleborate datatable to the other datatable using the join datatable adaptor.\n19) We now want to export this data to Microreact to visualise it. Before we do this. We need to convert this datatable to CSV form which can be done using the export to CSV file adaptor.\n\n\n\n\n\n20) Select the export to microreact adaptor.\n\nLink your CSV output of the previous adaptor to the data file input of this adaptor\nUnder the tree file input, add a new data-flo output for your tree file.\n\n\n\n\n\n\nOn a different browser tab, get your microreact API access token at https://microreact.org/my-account/settings (you must already have created your microreact account).\nGo to the My account tab at the top of your Microreact page and then to the Account Settings. Copy your API token.\n\n\n\n\n\nGo back to your Data-flo workflow and click on *access token and add your API token as a defined value.\nIn the outputs of the export to microreact click on URL and check the “Use as Workflow Output”\n\nRun your workflow\nOn the top right corner of your canvas, select the Run option.\nTo run the workflow upload the files named lab_results.xlsx, tree.nwk, epi_data.csv, and mlst-Pasteur.csv.\n\n\n\n\n\nClick on Run. The Outputs box now shows the url of a Microreact project created by data-flo.\n\n\n\n\n\nThe materials provided in this repository are FREE to use. This work is licensed under a Creative Commons Attribution 4.0 International License. Reuse is encouraged with acknowledgement."
  },
  {
    "objectID": "agenda.html",
    "href": "agenda.html",
    "title": "Agenda",
    "section": "",
    "text": "The detailed agenda can be found here."
  },
  {
    "objectID": "module6/microreact/microreact.html",
    "href": "module6/microreact/microreact.html",
    "title": "Microreact tutorial",
    "section": "",
    "text": "For Microreact documentation, go to https://docs.microreact.org/"
  },
  {
    "objectID": "module6/microreact/microreact.html#task-1-create-an-editable-project.",
    "href": "module6/microreact/microreact.html#task-1-create-an-editable-project.",
    "title": "Microreact tutorial",
    "section": "Task 1: Create an editable project.",
    "text": "Task 1: Create an editable project.\n\n\n\nTask 1: Create an editable project. Step 1: Notice the crossed out pen on right top corner of your screen. This indicates that you cannot edit the current Microreact project.\n\n\n\n\n\nTask 1: Create an editable project. Step 2: Click the crossed out “Pen” symbol in the top right of the screen. A window appears asking you to “SIGN IN TO EDIT”.\n\n\n\n\n\nTask 1: Create an editable project. Step 3: Once you sign in, the message changes, and you are invited to “MAKE A COPY” of the current project. Make a copy, to proceed.\n\n\n\n\n\nTask 1: Create an editable project. Step 4: Notice that, once you made a copy of the project, the crossed out “pen” symbol will change to a “normal pen”, and you will be able to edit and save the project."
  },
  {
    "objectID": "module6/microreact/microreact.html#task-2-present-whole-genome-sequencing-quality-control-wgs-qc-statistics-in-a-chart.",
    "href": "module6/microreact/microreact.html#task-2-present-whole-genome-sequencing-quality-control-wgs-qc-statistics-in-a-chart.",
    "title": "Microreact tutorial",
    "section": "Task 2: Present whole genome sequencing quality control (WGS QC) statistics in a chart.",
    "text": "Task 2: Present whole genome sequencing quality control (WGS QC) statistics in a chart.\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 1: Select the “Kpn Colombia” view. Click on the “Pen” symbol on the top right menu. Click on the “Create New Chart”\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 2: Drag the new chart to overlap with the tree.\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 3: An empty panel for the new chart will be shown on top of the tree panel.\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 4: In the Chart Type dropdown list select “Bar Chart”.\n\n\n\n\n\nTask2: Present WGS QC statistics in a chart. Step 5: A new window appears. In the X Axis Column, select “WGS_QC_no_contigs” and for “Maximum number of bins” select 10.\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 6: The bar chart will look like above. Observe that most genomes have less the 100 contigs."
  },
  {
    "objectID": "module6/microreact/microreact.html#task-3-what-are-the-dominating-sequence-types-sts-in-colombia",
    "href": "module6/microreact/microreact.html#task-3-what-are-the-dominating-sequence-types-sts-in-colombia",
    "title": "Microreact tutorial",
    "section": "Task 3: What are the dominating sequence types (STs) in Colombia?",
    "text": "Task 3: What are the dominating sequence types (STs) in Colombia?\n\n\n\nTask 3: What are the dominating STs in Colombia? Now that you’ve created one chart, you can create another one! Step 1: Go to the “Pen: symbol on the right hand side and click on the”Create New Chart”.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 2: The new chart can stay right on top of the previously created one.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 3: Notice a white canvas on top of the previously generated chart.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 4: Once again, from the Chart Type dropdown menu, select “Bar Chart”, and when the new view shows up on the “X Axis Column”, select ST.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 5: A new chart will appear. The labels on the x-axis appear squished and they are hard to read. Drag the panel divider on the left hand side of the chart, to increase the width of the panel.\n\n\n\n\n\nTask 3: What are the dominant STs in Colombia? Step 6: The information on the x-axis should be readable now. The 3 most abundant STs are ST11, ST258 and ST512.\n\n\n\n\n\nTask 3: What are the dominant STs in Colombia? Step 7: Click the “Views” panel on the left hand side, hover over “Kpn Colombia”, click on the three dots on the corner of the view and hit “Update View”\n\n\n\n\n\nTask 3: What are the dominant STs in Colombia? Step 8: Go to the Save icon on the right corner, press the icon and choose “Update This Project”"
  },
  {
    "objectID": "module6/microreact/microreact.html#task-4-plot-metadata-blocks-for-the-carbapenamase-genes-ctx-m-15-ndm-1-kpc-and-oxa.-what-are-the-prevalent-amr-mechanisms-detected",
    "href": "module6/microreact/microreact.html#task-4-plot-metadata-blocks-for-the-carbapenamase-genes-ctx-m-15-ndm-1-kpc-and-oxa.-what-are-the-prevalent-amr-mechanisms-detected",
    "title": "Microreact tutorial",
    "section": "Task 4: Plot metadata blocks for the carbapenamase genes CTX-M-15, NDM-1, KPC and OXA. What are the prevalent AMR mechanisms detected ?",
    "text": "Task 4: Plot metadata blocks for the carbapenamase genes CTX-M-15, NDM-1, KPC and OXA. What are the prevalent AMR mechanisms detected ?\n\n\n\nTask 4: Plot metadata blocks for CTX-M-15, NDM-1, KPC and OXA genes. What are the prevalent AMR mechanisms detected ? Step 1: Press the icon on the top right of the tree panel.\n\n\n\n\n\nTask 4: Plot metadata blocks for CTX-M-15, NDM-1, KPC and OXA genes. What are the prevalent AMR mechanisms detected ? Step 2: In the Metadata blocks dropdown list, tick all boxes containing KPC, NDM, VIM, OXA and CTX-M-15 genes.\n\n\n\n\n\nTask 4: Plot metadata blocks for CTX-M-15, NDM-1, KPC and OXA genes. What are the prevalent AMR mechanisms detected ? Step 3: The tree panel will show metadata columns. Yellow indicates presence of a certain gene and green indicates absence. In the panel shown here, it appears that the most common genes present are KPC-2, KPC-3, NDM-1 and CTX-M-15."
  },
  {
    "objectID": "module6/microreact/microreact.html#task-5-which-sts-are-associated-with-the-presence-of-carbapenamase-genes",
    "href": "module6/microreact/microreact.html#task-5-which-sts-are-associated-with-the-presence-of-carbapenamase-genes",
    "title": "Microreact tutorial",
    "section": "Task 5: Which STs are associated with the presence of carbapenamase genes?",
    "text": "Task 5: Which STs are associated with the presence of carbapenamase genes?\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? Step 1: Go to the “Metadata blocks” and check the ST box.\n\n\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? Step 2: Observe the new metadata column next to the tree, with the header “ST”.\n\n\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? In the tree with added metadata blocks, we can observe a large brown block of isolates belonging to ST258. You will need to check the Legend on the very right of Microreact.\n\n\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? On a close look, we can observe that ST258 is associated with the presence of the carbapenamase gene KPC-3."
  },
  {
    "objectID": "module6/module6.html#create-a-microreact-project-from-a-google-spreadsheet",
    "href": "module6/module6.html#create-a-microreact-project-from-a-google-spreadsheet",
    "title": "SESSION 6 - Data visualisation and Interpretation with Microreact",
    "section": "2. Create a Microreact project from a Google Spreadsheet",
    "text": "2. Create a Microreact project from a Google Spreadsheet\n\nOpen this Google spreadsheet\n\n\n\nMake a copy of this in your own Google account by selecting Make a copy from the File menu item. When prompted, click on Make a copy. This will open a separate tab where the copy will be available.\n\n\n\n\nSet access to shareable by clicking on the Share button at the top right of the screen. A dialogue screen will pop up, here, click on the Restricted button, select Anyone with the link, and click on Done\n\n\n\nNow to publish the google spreadsheet click on the File menu item, select Share, and click on Publish on web\n\n\n\nIn the popup message click on Web page, and select Comma-separated values (.csv). Also, make sure the Automatically republish when changes are made option is turned on under the Published content and settings section\n\n\n\n\nConfirm your choices in the popup message clicking on OK. This will provide more details about your Google spreadsheet, copy the url.\n\n\n\n\nGo to microreact.org, and select Upload from the main menu.\n\n\n\nSelect the plus icon at the bottom right, and click on Add URLs\n\n\n\nPaste the url from step f and select Data (CSV or TSV) under File kind. Finally click on CONTINUE to see your microreact.\n\n\n\nYou should get a screen like this!\n\n\nThe materials provided in this repository are FREE to use. This work is licensed under a Creative Commons Attribution 4.0 International License. Reuse is encouraged with acknowledgement."
  },
  {
    "objectID": "module1/stakeholders-mapping.html",
    "href": "module1/stakeholders-mapping.html",
    "title": "Stakeholders mapping",
    "section": "",
    "text": "This module aims to cover Theory on stakeholder mapping; Practicalities on identifying stakeholders in each of the units and at the central hub; Practicalities on identifying Digital Epidemiology needs across different stakeholders.\nAt the end of this module participants will be able to: Recognize important stakeholders in own institution; Identify WHAT information needs to be communicated to them, WHEN and HOW.\nSuggested pre-workshop reading:\nFull mapping of the chain process for three main productions in E.U\nMapping food surveillance chains through different sectors, by Laura Amato et al, Front. Public Health, 18 April 2023"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Here you can find other Resources that are useful:\nhttps://mmbdtp.github.io/modules/sequencing/file-formats/\nhttps://training.nextflow.io/basic_training/\nhttps://www.protocols.io/view/ghru-genomic-surveillance-of-antimicrobial-resista-bp2l6b11kgqe/v4\nhttps://gitlab.com/cgps/ghru/pipelines/assembly"
  }
]